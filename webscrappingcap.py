# -*- coding: utf-8 -*-
"""WebScrappingCap.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b_ranX_e2qc1lSQ13D3-YMfRUFXUHIEe

**1.Install Required Libraries**
"""

!pip install requests beautifulsoup4 pandas lxml

"""**2.Import the Required Libraries**



 1.from requests import get\
2.from bs4 import BeautifulSoup\
3.import pandas as pd

**3.Select the Website to Scrape**
Choose the website you want to scrape.To scrape Wikipedia, I'll use a URL like:\
url = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"

**4.Send a GET Request to the Website**\
Use the requests.get() method to fetch the webpage content.

response = get(url)

**5.Check the Status Code**\
Ensure the request was successful (status code 200)\

---
if response.status_code == 200:\
    * print("Successfully fetched the web page!")*\
else:\
    print(f"Failed to retrieve the web page. Status code: {response.status_code}")

**6.Parse the HTML Content**\
Parse the HTML content of the webpage using BeautifulSoup:\

table = soup.find("table", {"class": "wikitable"})
"""



import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL of the website to scrape (Wikipedia example)
url = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"

# Send a GET request to fetch the webpage content
response = requests.get(url)
if response.status_code == 200:
    print("Successfully fetched the web page!")
else:
    print(f"Failed to retrieve the web page. Status code: {response.status_code}")

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.text, "lxml")

# Find the specific table that contains the data we need
table = soup.find("table", {"class": "wikitable"})

# Initialize lists to hold the data
countries = []
populations = []

# Iterate over table rows and extract country names and population data
for row in table.find_all("tr")[1:]:
    cells = row.find_all("td")
    if len(cells) > 1:
        country = cells[1].get_text(strip=True)
        population = cells[2].get_text(strip=True)
        countries.append(country)
        populations.append(population)

# Create a DataFrame using pandas
df = pd.DataFrame({
    "Country": countries,
    "Population": populations
})

# Preview the data
print(df.head(100))

!pip install matplotlib

import matplotlib.pyplot as plt

df.head()

# Save the data to a CSV file
df.to_csv("countries_population.csv", index=False)
print("Data saved as countries_population.csv")

# Save the data to a JSON file
df.to_json("countries_population.json", orient="records")
print("Data saved as countries_population.json")

# Save the data to an Excel file
df.to_excel("countries_population.xlsx", index=False)
print("Data saved as countries_population.xlsx")

"""From this code we fetch some extra column and save them to csv and xlsx file \
1.Density\
2.Region\
3.Area\
4.Population\
5.Country
"""

from requests import get
from bs4 import BeautifulSoup
import pandas as pd

# URL of the website to scrape (Wikipedia example)
url = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"

# Send a GET request to fetch the webpage content
response = get(url)
if response.status_code == 200:
    print("Successfully fetched the web page!")
else:
    print(f"Failed to retrieve the web page. Status code: {response.status_code}")

# Parse the HTML content using BeautifulSoup
soup = BeautifulSoup(response.text, "lxml")

# Find the specific table that contains the data we need
table = soup.find("table", {"class": "wikitable"})

# Initialize lists to hold the data
countries = []
populations = []
regions = []
areas = []
densities = []

# Iterate over table rows and extract country names, population data, region, area, and density
for row in table.find_all("tr")[1:]:
    cells = row.find_all("td")
    if len(cells) > 1:
        country = cells[1].get_text(strip=True)
        population = cells[2].get_text(strip=True)
        region = cells[3].get_text(strip=True)
        area = cells[4].get_text(strip=True)
        density = cells[5].get_text(strip=True)

        countries.append(country)
        populations.append(population)
        regions.append(region)
        areas.append(area)
        densities.append(density)

# Create a DataFrame using pandas
df = pd.DataFrame({
    "Country": countries,
    "Population": populations,
    "Region": regions,
    "Area (km²)": areas,
    "Density (per km²)": densities
})

# Preview the data
print(df.head(100))

df.head()

"""Fetch data from website and convert them into CSV File

"""

# Save the data to a CSV file
df.to_csv("countries_population.csv", index=False)
print("Data saved as countries_population.csv")

"""Fetch data from website and make download them into Json Format"""

# Save the data to a JSON file
df.to_json("countries_population.json", orient="records")
print("Data saved as countries_population.json")

"""Fetch data from website and make download them into XLSX Format"""

# Save the data to an Excel file
df.to_excel("countries_population.xlsx", index=False)
print("Data saved as countries_population.xlsx")